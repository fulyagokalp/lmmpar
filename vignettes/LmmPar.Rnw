%% \VignetteIndexEntry{Some improved procedures for linear mixed models}
%% \Vignettekeywords{linear mixed model}
%% \VignettePackage{lmm}

\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}


%Bibliography packages
%\usepackage{natbib}
\usepackage[utf8]{inputenc}
%\usepackage[english]{babel}
\usepackage[nottoc]{tocbibind}

%Define variables and parameters
\newcommand{\uA}       {\mbox{\boldmath$A$}}
\newcommand{\uD}       {\mbox{\boldmath$D$}}
\newcommand{\uU}       {\mbox{\boldmath$U$}}
\newcommand{\uu}       {\mbox{\boldmath$u$}}
\newcommand{\uR}       {\mbox{\boldmath$R$}}
\newcommand{\uW}       {\mbox{\boldmath$W$}}
\newcommand{\uX}       {\mbox{\boldmath$X$}}
\newcommand{\uy}       {\mbox{\boldmath$y$}}
\newcommand{\uZ}       {\mbox{\boldmath$Z$}}
\newcommand{\ualpha}            {\mbox{\boldmath$\alpha$}}
\newcommand{\ubeta}             {\mbox{\boldmath$\beta$}}
\newcommand{\ugamma}            {\mbox{\boldmath$\gamma$}}
\newcommand{\udelta}            {\mbox{\boldmath$\delta$}}
\newcommand{\uepsilon}          {\mbox{\boldmath$\epsilon$}}
\newcommand{\utheta}            {\mbox{\boldmath$\theta$}}

\begin{document}
\SweaveOpts{concordance=TRUE}

\title{Linear Mixed Models with Parallel Programming %\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
\date{}
\maketitle

\section{Linear Mixed Models}
\label{LMM}
The LMM is defined for a continuous response variable as


\begin{align}
\begin{aligned}\label{eq:1}
\mathbf{y}_i&= \mathbf{X}_i \mathbf{\beta}+ \mathbf{Z}_i \mathbf{u}_i+ \mathbf{e}_i  , i = 1,2...n,\\
\mathbf{u}_i&\sim \mathrm{N}_q(\mathbf{0},\mathbf{\psi}),\\
\mathbf{e}_i  &\sim \mathrm{N}_{n_i } (\mathbf{0},\mathbf{R}_i),\\
\end{aligned}
\end{align}


where $\mathbf{y}_i$ denotes an $n_i$ dimensional vector of continuous responses for the i-th subject, $\mathbf{\beta}$ denotes a $p$ dimensional vector of unknown population parameters, $\mathbf{u}_i$  denotes a $q$ dimensional vector of unknown individual effects, $\mathbf{X}_i$ is a known $(n_i \times p)$ design matrix,  $\mathbf{Z}_i$ is a known $(n_i \times q)$ design matrix, $\mathbf{e}_i$ denotes an $n_i$ dimensional vector of residual errors assumed to be independent of $\mathbf{u}_i$. $\psi$ is a $(q \times q)$ covariance matrix of random effects; while $\mathbf{R}_i$ is a $(n_i \times n_i)$ covariance matrix of residual errors \cite{Laird1982}. It is often assumed that $\mathbf{R}_i= \sigma^2 \mathbf{I}_{n_i}$ for simplicity. $\sigma^2$ and $\mathbf{\psi}$ are positive definite.

The marginal distribution of the response variable is obtained as

\begin{align}
\begin{aligned}\label{eq:2}
\mathbf{y}_i \sim N_{n_i}(\mathbf{X}_i\mathbf{\beta}, \sigma^2\mathbf{W}_i^{-1})
\end{aligned},
\end{align}

where $\mathbf{W}_i=(\mathbf{Z}_i\mathbf{D}\mathbf{Z}_i+\mathbf{R}_i)^{-1}$ and $\mathbf{D}=\sigma^{-2}\psi$. Maximizing the following log-likelihood function of (\ref{eq:2}) brings the maximum-likelihood (ML) estimates of $\mathbf{\beta}, \mathbf{\sigma^2}$ and $\mathbf{D}$

\begin{equation}\label{eq:3}
\mathrm{L}_0(\mathbf{\beta}, \sigma^2, \mathbf{D}) \propto (\mathbf{\sigma}^2)^{-N/2} \prod_{i=1}^n |\mathbf{W}_i|^{1/2} exp\left\lbrace  -\frac{1}{2\sigma^2} (\mathbf{y}_i-\mathbf{X}_i\mathbf{\beta})^T\mathbf{W}_i(\mathbf{y}_i-\mathbf{X}_i\mathbf{\beta}) \right\rbrace
\end{equation}

where $N = \sum_{i=1}^n n_i$.

Given $\mathbf{y}=(\mathbf{y}_1, \mathbf{y}_, ..., \mathbf{y}_n)$ and $(\beta, \sigma^2, \mathbf{D})$, random effects $\mathbf{u}_1,...,\mathbf{u}_n$ are independently normally distributed with following moments

\begin{equation}\label{eq:4}
\mathrm{E}(\uu_i | \uy,\ubeta, \sigma^2, \uD) = \uU_i \uZ_i^T \uR_i^{-1}(\uy_i - \uX_i \ubeta)
\end{equation}

\begin{equation}\label{eq:5}
\mathrm{V}(\uu_i | \uy,\ubeta, \sigma^2, \uD) = \sigma^2 \uU_i
\end{equation}
where
\begin{equation}\label{eq:6}
\uU_i = (\uD^{-1} + \uZ_i^T \uR_i^{-1} \uZ_i)^{-1}
\end{equation}

Detailed proofs and empirical Bayes point and interval estimates for random effects are found in \cite{Schafer1998}.

There are two commonly used algorithms to maximize (\ref{eq:3}), which are Expectation Maximization (EM) and Newton-Raphson (NR) algorithms. EM is easy and stable. Also, since it is proven that the likelihood increases at each iteration of EM, we prefer to use an EM-type of algorithm, in this study.

\section{ECME Algorithm}
\label{ECME}

In a general scheme of EM for LMM, random effects are treated as missing data and their conditional expectations are evaluated with (\ref{eq:4}). Since maximization part is composed of fixing some unknown parameters and updating the rest of them and also fixing the parameters and updating the expectations, the algorithm used in this study is no longer EM, it is called Expectation/Conditional Maximization Either (ECME) \cite{LiuRubin1994}.

We use the similar algorithm used in \cite{Schafer1998} which is the main reference of `lmm' package in R.

\begin{equation}\label{eq:7}
\uU_i^{(k)} = (\uD^{-1(k)} + \uZ_i^T \uR_i^{-1} \uZ_i)^{-1},
\end{equation}
\begin{equation}\label{eq:8}
\uW_i^{(k)} = \uR_i^{-1} - \uR_i^{-1} \uZ_i \uU_i^{(k)} \uZ^T \uR_i^{-1},
\end{equation}
\begin{equation}\label{eq:9}
\ubeta^{(k)} = \left(\sum_{i=1}^n \uX_i^T \uW_i^{(k)} \uX_i
\right)^{-1}
\left(\sum_{i=1}^n \uX_i^T \uW_i^{(k)} \uy_i
\right),
\end{equation}
\begin{equation}\label{eq:10}
\sigma_i^{2(k+1)} = \frac{1}{\mathrm{N}}\sum_{i=1}^n (\uy_i-\uX_i \ubeta^{(k)})\uW_i^{(k)}(\uy_i-\uX_i \ubeta^{(k)}),
\end{equation}
\begin{equation}\label{eq:11}
\uu_i^{(k)} = \uU_i^{(k)} \uZ_i^T \uR_i^{-1} (\uy_i - \uX_i \ubeta^{(k)}),
\end{equation}
\begin{equation}\label{eq:12}
\uD^{(k+1)} = \frac{1}{n} \sum_{i=1}^n (\sigma^{-2(k)} \uu_i^{(k)} \uu_i^{(k)T} + \uU_i^{(k)}).
\end{equation}

The likelihood evaluated at cycle (k) is as following

\begin{align}
\begin{aligned}\label{eq:13}
\mathrm{L}(\ubeta^{(k)}, \sigma^{2(k)}, \uD^{(k)}) &= -\frac{N}{2} log\sigma^{2(k)} -\frac{n}{2}log|\uD^{(k)}|\\ &+\frac{1}{2} \sum_{i=1}^nlog|\uU_i^{(t)}| -\frac{N}{2} \left( \frac{\sigma^{2(k+1)}}{\sigma^{2(k)}} \right).
\end{aligned}
\end{align}

The ECME algorithm updates unknown parameters which are $\ubeta, \sigma^2, \uD$ at each step until converges \cite{Schafer1998}.

\section{Linear Mixed Models with Parallel Programming}
\label{LMMPar}

The ECME algorithm introduced at the previous section is implemented under the R-environment in divide-and-combine structure. Steps 7,8,10 and 11 are implemented for each observation, and steps 9, 10 and 11 are mainly average values of some combinations of these values with observed data. We aim to divide these calculations and send corresponding parts of them segmentally to the several nodes of the system.

For each $c = 1,2,...,C$, where $C$ is the maximum number of nodes, we perform calculations for 7,8,10 and 11. In other words, the data are divided into $C$ parts and E-step is implemented at each node since each observation/subject is independent of each other given a current estimate. (Repeats are dependent, since they are gathered from the same subject/observation).

The data $\uA$ is divided into $C$ splits. Let $\eta$ and $\utheta = (\ubeta, \sigma^2, \uD)$ show the sufficient statistics and parameters, respectively and the sufficient statistics for given parameters are:

\begin{equation}\label{eq:14}
\eta^{(i)} = \mathrm{E}_{\theta}(\eta | \uA_i).
\end{equation}

For the first node, the first sufficient statistic is calculated as

\begin{equation}\label{eq:15}
\eta_1^{(i)} = \uX_i^T \uW_i^{(current)} \uX_i,
\end{equation}

and the first combine is

\begin{equation}\label{eq:16}
\eta_1^{(c)} = \sum_{i=1}^{N/C}\uX_i^T \uW_i^{(current)} \uX_i.
\end{equation}

Note that for the j. combine, $i$ is defined from $[(j-1)\frac{\mathrm{N}}{C}+1]$ to $[j\frac{\mathrm{N}}{C}]$. For the ease of notation, we assume that the number of nodes is the exact divisor of the total number of observations. If not, the system  optimally sets it.

The final combine for the first sufficient statistics is

\begin{equation}\label{eq:17}
\eta_1 = \sum_{j=1}^{C} \eta_1^{(j)}.
\end{equation}

Depending on the notation of (\ref{eq:17}), $\ubeta$ will be

\begin{equation}\label{eq:18}
\ubeta = (\eta)^{-1}\eta_2.
\end{equation}

More explicitly,

\begin{equation}\label{eq:19}
\hat{\ubeta} = \left( \sum_{j=1}^C \sum_{i=[(j-1)\frac{\mathrm{N}}{C}+1]}^{[j\frac{\mathrm{N}}{C}]}\uX_{ji}^T \hat{\uW}_{ji} \uX_{ji} \right) ^{-1} \left( \sum_{j=1}^C \sum_{i=[(j-1)\frac{\mathrm{N}}{C}+1]}^{[j\frac{\mathrm{N}}{C}]}\uX_{ji}^T \hat{\uW}_{ji} \uX_{ji} \right).
\end{equation}

Similarly,

\begin{equation}\label{eq:20}
\hat{\sigma}^2 = \frac{1}{N} \sum_{j=1}^C \sum_{i=[(j-1)\frac{\mathrm{N}}{C}+1]}^{[j\frac{\mathrm{N}}{C}]} (\uy_{ji}-\uX_{ji} \hat{\ubeta}) \hat{\uW}_{ji} (\uy_{ji}-\uX_{ji} \hat{\ubeta})
\end{equation}

\begin{equation}\label{eq:21}
\hat{\uD} = \frac{1}{n} \sum_{j=1}^C \sum_{i=[(j-1)\frac{\mathrm{N}}{C}+1]}^{[j\frac{\mathrm{N}}{C}]} (\hat{\sigma}^2 \uu_i \uu_i^T + \uU_i).
\end{equation}

Note that (\ref{eq:19}), (\ref{eq:20}), and (\ref{eq:21}) include the estimations of $\uU_i, \uW_i, and \uu_i$.

The different processes are currently managed by the `parallel` package using `parallel::makeCluster(cores)` and `parallel::clusterApply()`.  Other common packages such as `plyr` and `foreach` wrap around the `parallel` package using the `doParallel` package.  By calling `parallel` directly, we avoid any computational overhead and unnecessary dependencies.




%NOT SURE TO ADD FOLLOWING

%The joint distribution of equation (1) for $(\mathbf{y}_i^T,\mathbf{u}_i^T )^T$ is
%
%
%\begin{equation}\label{eq:2}
%\left[
%\begin{array}{cc}\mathbf{y}_i\\\mathbf{u}_i\end{array}
%\right] \sim \mathrm{N}_{n_i+q} \left( \left[
%\begin{array}{cc}\mathbf{X}_i\mathbf{\beta}\\0
%\end{array}
%\right], \left[
%\begin{array}{cc}
%\mathbf{Z}_i\mathbf{\psi}\mathbf{Z}_i^T+\mathbf{R}_i&\mathbf{Z}_i\mathbf{\psi}\\
%\mathbf{\psi}\mathbf{Z}_i^T&\mathbf{\psi}
%\end{array}
%\right]\right).
%\end{equation}



\medskip
% BibTeX users please use one of
\bibliographystyle{unsrt}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{MyCollection_LMMPar}   % name your BibTeX data base




\end{document}
